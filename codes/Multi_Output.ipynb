{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchmetrics.functional import dice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_train import TorchTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/kaggle/input/google-research-identify-contrails-reduce-global-warming\"\n",
    "DEVICE = TorchTrain.DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record_ID</th>\n",
       "      <th>Split</th>\n",
       "      <th>Contrails</th>\n",
       "      <th>High_Pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000216489776414077</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000603527582775543</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>8.512878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000660467359258186</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>1.028442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071707854144929</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000823728928031783</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0.563049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Record_ID  Split  Contrails  High_Pixels\n",
       "0  1000216489776414077  train      False     0.000000\n",
       "1  1000603527582775543  train       True     8.512878\n",
       "2  1000660467359258186  train       True     1.028442\n",
       "3   100071707854144929  train      False     0.000000\n",
       "4  1000823728928031783  train       True     0.563049"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record_ID</th>\n",
       "      <th>Split</th>\n",
       "      <th>Contrails</th>\n",
       "      <th>High_Pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20529</th>\n",
       "      <td>1000834164244036115</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20530</th>\n",
       "      <td>1002653297254493116</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20531</th>\n",
       "      <td>1002777035567823518</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.038147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20532</th>\n",
       "      <td>1010397530434035516</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20533</th>\n",
       "      <td>1012978360687713914</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Record_ID       Split  Contrails  High_Pixels\n",
       "20529  1000834164244036115  validation      False     0.000000\n",
       "20530  1002653297254493116  validation      False     0.000000\n",
       "20531  1002777035567823518  validation       True     0.038147\n",
       "20532  1010397530434035516  validation      False     0.000000\n",
       "20533  1012978360687713914  validation      False     0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = pd.read_csv(\"metadata.csv\")\n",
    "train_md = metadata[metadata[\"Split\"] == \"train\"]\n",
    "validation_md = metadata[metadata[\"Split\"] == \"validation\"]\n",
    "display(train_md.head())\n",
    "display(validation_md.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_T11_BOUNDS = (243, 303)\n",
    "_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n",
    "_TDIFF_BOUNDS = (-4, 2)\n",
    "N_TIMES_BEFORE = 4\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def normalize_range(data, bounds):\n",
    "    \"\"\"Maps data to the range [0, 1].\"\"\"\n",
    "    return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "\n",
    "def load_one_record(record_id, BASE_DIR=BASE_DIR, mask_too=True):\n",
    "    band11 = np.load(f\"{BASE_DIR}/{str(record_id)}/band_11.npy\")\n",
    "    band14 = np.load(f\"{BASE_DIR}/{str(record_id)}/band_14.npy\")\n",
    "    band15 = np.load(f\"{BASE_DIR}/{str(record_id)}/band_15.npy\")\n",
    "    if mask_too:\n",
    "        human_pixel_mask =  np.load(f\"{BASE_DIR}/{str(record_id)}/human_pixel_masks.npy\")\n",
    "        \n",
    "    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n",
    "    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "    b = normalize_range(band14, _T11_BOUNDS)\n",
    "    false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "    \n",
    "    img = false_color[..., N_TIMES_BEFORE]\n",
    "    if mask_too:\n",
    "        return img, human_pixel_mask\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22586/4247539866.py:3: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  contrails_presnt = torch.tensor(contrails_presnt).to(torch.float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (int, int, int), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m images, mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m3\u001b[39m)), torch\u001b[39m.\u001b[39mones((\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[39m# images = torch.tensor(np.reshape(images, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# images = self.normalize_image(images)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39;49mtranspose(\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcatenate((mask, contrails_presnt\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (int, int, int), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "record_id = train_md.iloc[idx][\"Record_ID\"]\n",
    "contrails_presnt = train_md.iloc[idx][\"Contrails\"]\n",
    "contrails_presnt = torch.tensor(contrails_presnt).to(torch.float32)\n",
    "images, mask = torch.ones((256, 256, 3)), torch.ones((256, 256, 1))\n",
    "# images = torch.tensor(np.reshape(images, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n",
    "# images = self.normalize_image(images)\n",
    "images = images.transpose(2, 0, 1)\n",
    "mask = mask.transpose(2, 0, 1)\n",
    "outputs = torch.concatenate((mask, contrails_presnt.unsqueeze(0)), dim=0)\n",
    "images.float(), outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 4 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m tensor2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Concatenate the two tensors along the first dimension\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((tensor1, tensor2), dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 4 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(2, 2)\n",
    "\n",
    "# Concatenate the two tensors along the first dimension\n",
    "tensor = torch.cat((tensor1, tensor2), dim=0)\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones((256, 256, 1))\n",
    "classes = np.ones((1, 1))\n",
    "mask = torch.tensor(mask)\n",
    "classes = torch.tensor(classes)\n",
    "outputs = (mask, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "outputs.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.ones((256, 256, 1))\n",
    "# classes = [[1]]\n",
    "# outputs = [mask, classes]\n",
    "# output = torch.tensor([mask, classes])\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrailsDataLoader():\n",
    "    def __init__(self, df, split=\"train\"):\n",
    "        self.df = df\n",
    "        self.split = split\n",
    "        self.normalize_image = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        record_id = self.df.iloc[idx][\"Record_ID\"]\n",
    "        contrails_presnt = self.df.iloc[idx][\"Contrails\"]\n",
    "        contrails_presnt = torch.tensor(contrails_presnt).to(torch.float32)\n",
    "        images, mask = load_one_record(record_id, f\"{BASE_DIR}/{self.split}\")\n",
    "        images = torch.tensor(np.reshape(images, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n",
    "        images = self.normalize_image(images)\n",
    "        mask = mask.transpose(2, 0, 1)\n",
    "        outputs = torch.concatenate((mask, contrails_presnt.unsqueeze(0)), dim=0)\n",
    "        return images.float(), outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "class ContrailsDataLoaderTest():\n",
    "    def __init__(self, ids, split=\"test\"):\n",
    "        self.ids = ids\n",
    "        self.split = split\n",
    "        self.normalize_image = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        record_id = self.ids[idx]\n",
    "        images = load_one_record(record_id, f\"{BASE_DIR}/{self.split}\", mask_too=False)\n",
    "        images = torch.tensor(np.reshape(images, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n",
    "        images = self.normalize_image(images)\n",
    "        return images.float(), record_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = ContrailsDataLoader(train_md, \"train\")\n",
    "validation_dataloader = ContrailsDataLoader(validation_md, \"validation\")\n",
    "\n",
    "train_data = DataLoader(train_dataloader, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "validation_data = DataLoader(validation_dataloader, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithLogitsLossUpdated:\n",
    "    def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None):\n",
    "        self.weight = weight\n",
    "        self.size_average = size_average\n",
    "        self.reduce = reduce\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def __call__(self, inputs, targets):\n",
    "        mask_pred, class_pred = inputs\n",
    "        mask_true, class_true = targets\n",
    "        mask_loss = F.binary_cross_entropy_with_logits(mask_pred, mask_true, self.weight, self.size_average, self.reduce, self.reduction, self.pos_weight)\n",
    "        class_loss = F.binary_cross_entropy_with_logits(class_pred, class_true, self.weight, self.size_average, self.reduce, self.reduction, self.pos_weight)\n",
    "        return mask_loss + class_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
